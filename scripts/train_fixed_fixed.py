import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib
import os
import click


@click.command()
@click.option('--features-csv', required=True, help='CSV —Ñ–∞–π–ª —Å —Ñ–∏—á–∞–º–∏ –∑–¥–∞–Ω–∏–π')
@click.option('--train-csv', required=True, help='CSV —Ñ–∞–π–ª —Å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏')
@click.option('--model-save-path', default='models/population_model.pkl', help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏')
def main(features_csv, train_csv, model_save_path):
    print("="*60)
    print("–¢–†–ï–ù–ò–†–û–í–ö–ê –ú–û–î–ï–õ–ò –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –ù–ê–°–ï–õ–ï–ù–ò–Ø")
    print("="*60)

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\n1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

    if not os.path.exists(features_csv):
        print(f"‚ùå –§–∞–π–ª —Å —Ñ–∏—á–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {features_csv}")
        return

    if not os.path.exists(train_csv):
        print(f"‚ùå –§–∞–π–ª —Å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {train_csv}")
        return

    df_features = pd.read_csv(features_csv)
    df_train = pd.read_csv(train_csv)

    print(
        f"   –§–∏—á–∏: {df_features.shape[0]} —Å—Ç—Ä–æ–∫, {df_features.shape[1]} –∫–æ–ª–æ–Ω–æ–∫")
    print(
        f"   –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {df_train.shape[0]} —Å—Ç—Ä–æ–∫, {df_train.shape[1]} –∫–æ–ª–æ–Ω–æ–∫")

    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\n2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

    # –ï—Å–ª–∏ —Ñ–∞–π–ª—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–∏–Ω DataFrame
    if features_csv == train_csv:
        df = df_features
    else:
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ building_id
        if 'building_id' in df_features.columns and 'building_id' in df_train.columns:
            df = pd.merge(df_features, df_train[[
                          'building_id', 'population']], on='building_id', how='inner')
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç building_id, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã
            df = df_features.copy()
            df['population'] = df_train['population'].values[:len(df)]

    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ –Ω–∞—Å–µ–ª–µ–Ω–∏—è
    initial_count = len(df)
    df = df.dropna(subset=['population'])
    removed_count = initial_count - len(df)

    print(
        f"   –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è NaN –≤ population: {len(df)} —Å—Ç—Ä–æ–∫ (—É–¥–∞–ª–µ–Ω–æ {removed_count})")

    if len(df) == 0:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏—á–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    exclude_cols = ['building_id', 'population']
    feature_cols = [col for col in df.columns if col not in exclude_cols]

    X = df[feature_cols]
    y = df['population']

    print(f"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ (X): {X.shape[1]}")
    print(f"   –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (y): {y.shape[0]} –∑–Ω–∞—á–µ–Ω–∏–π")

    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º y.mean() –≤ float
    try:
        y_mean = float(y.mean())
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –Ω–∞—Å–µ–ª–µ–Ω–∏–µ: {y_mean:.2f}")
    except Exception as e:
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –Ω–∞—Å–µ–ª–µ–Ω–∏–µ: {y.mean():.2f}")

    # 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    print("\n3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")
    print(f"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤")

    # 4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\n4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ RandomForest...")
    model = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )

    model.fit(X_train, y_train)
    print("   ‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞")

    # 5. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    print("\n5. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...")
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    print(f"   MAE: {mae:.4f}")
    print(f"   RMSE: {rmse:.4f}")
    print(f"   R¬≤: {r2:.4f}")

    # 6. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\n6. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Ç–æ–ø-10):")
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)

    for i, row in feature_importance.head(10).iterrows():
        print(f"   {row['feature']}: {row['importance']:.4f}")

    # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\n7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
    joblib.dump(model, model_save_path)
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_save_path}")

    # 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    metrics = {
        'mae': float(mae),
        'rmse': float(rmse),
        'r2': float(r2),
        'n_samples': int(len(df)),
        'n_features': int(len(feature_cols))
    }

    metrics_path = model_save_path.replace('.pkl', '_metrics.json')
    import json
    with open(metrics_path, 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"   ‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metrics_path}")

    print("\n" + "="*60)
    print("üéâ –¢–†–ï–ù–ò–†–û–í–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
    print("="*60)


if __name__ == '__main__':
    main()
